# P3：大语言模型面试题系列（三） - AI大模型知识分享 - BV1UkiiYmEB9

今天给大家带来的面试题目呢，是目前主流的大语言模型的。

![](img/35d23956928480939c04a4fdf5c355b6_1.png)

开源模型体系有哪些，那有可能面试官还会这么问。

![](img/35d23956928480939c04a4fdf5c355b6_3.png)

就是preface decoder，causal decoder和encoder decoder。

![](img/35d23956928480939c04a4fdf5c355b6_5.png)

它们之间的区别是什么，我们知道在预训练语言模型时代呢。

![](img/35d23956928480939c04a4fdf5c355b6_7.png)

基本上采用的是预训练加微调的范式，这个时候呢其实bird是发展最好的。

![](img/35d23956928480939c04a4fdf5c355b6_9.png)

那么BT主要是通过encoder older来实现的。

![](img/35d23956928480939c04a4fdf5c355b6_11.png)

还有GPT代表的是decoder only，额，那么除了这个之外，还有就是encoder加decoder。



![](img/35d23956928480939c04a4fdf5c355b6_13.png)

这个时候呢是以T5为代表的，随着这个GPT系列的模型的成功发展呢。

![](img/35d23956928480939c04a4fdf5c355b6_15.png)

啊，这个时候走向了一个深层式的大语言，模型的道路。

![](img/35d23956928480939c04a4fdf5c355b6_17.png)

此时的话又把这个架构啊做了一些拆解，此时主要针对解码器架构呢做了两个变种，一种呢叫causal decoder。



![](img/35d23956928480939c04a4fdf5c355b6_19.png)

其实主要是啊叫因果解码器，另外一个是prefix decoder。

![](img/35d23956928480939c04a4fdf5c355b6_21.png)

主要是前缀解码器，然后下面这个图里面的话，主要是针对cause of decoder。

![](img/35d23956928480939c04a4fdf5c355b6_23.png)

prefix decoder和encoder decoder的一个啊简单的对比。

![](img/35d23956928480939c04a4fdf5c355b6_25.png)

大家先针对这三个图呢有一个初步的了解，然后后面会给大家。

![](img/35d23956928480939c04a4fdf5c355b6_27.png)

针对这几个图做一个详细的介绍，我们呃呃如果说对之前的模型架构熟悉的话，对encoder decoder的模型架构会稍微熟悉一点。



![](img/35d23956928480939c04a4fdf5c355b6_29.png)

所以我们先给大家介绍一下关于这个encoder，Decoder，encoder decoder架构呢，其实主要是基于transformer这边来啊，变化而来的，那么整体里面的话，它的架构是在编码器端。

采用了双向的自注意力机制，对输入信息进行编码处理，那么在解码器端呢采用了交叉注意力和掩码。

![](img/35d23956928480939c04a4fdf5c355b6_31.png)

自注意力机制，通过这种自回归的方式来进行输出，这种编码器，解码器这种方式呢，其实在一些额理解任务里边性能是非常好的，然后现在主流的一些架构里面的话。



![](img/35d23956928480939c04a4fdf5c355b6_33.png)

呃，其实也就T5这边的话主要是通过这种方式的，大家可以看到在这个里面的话，这个是啊encoder的架构。



![](img/35d23956928480939c04a4fdf5c355b6_35.png)

而这个是decoder架构，也就是说我们在这个里边，它主要是通过这种双向的自注意力机制。

![](img/35d23956928480939c04a4fdf5c355b6_37.png)

对这些信息呢进行编码处理。

![](img/35d23956928480939c04a4fdf5c355b6_39.png)

那么在这块里面，它呢主要是使用这种交叉注意力机制和掩码。

![](img/35d23956928480939c04a4fdf5c355b6_41.png)

自注意力机制来完成的。

![](img/35d23956928480939c04a4fdf5c355b6_43.png)

讲完这块我们再看一下这个cause of decoder，casa decoder呢其实是目前主流的一些框架。



![](img/35d23956928480939c04a4fdf5c355b6_45.png)

它呢主要是通过GPT系列，来带活整个框架的内容的，然后它主要是使用的这种单向的注意力掩码。

![](img/35d23956928480939c04a4fdf5c355b6_47.png)

确保每个输入token呢，只如只能注意到之前的token和它本身。

![](img/35d23956928480939c04a4fdf5c355b6_49.png)

然后输入和输出token呢，通过decoder以相同的方式来进行处理。

![](img/35d23956928480939c04a4fdf5c355b6_51.png)

就如下面这个里边的这个举例来看啊。

![](img/35d23956928480939c04a4fdf5c355b6_53.png)

然后在灰色这边就是这块的内容，代表的是我们这个里边的话呃，这个token是看不到后面的这些这些这些内容的，然后就拿我们的这一行举例来说，比如说这个里面SURIC位呢。



![](img/35d23956928480939c04a4fdf5c355b6_55.png)

是可以看到前面的这个ADD，但是他是看不到后面这个of，因为他这边的话把这个给遮掩出来了，然后这种架构，现在主要代表的其实就是GPT3，然后GPT系列的模型里边呃，还有拉玛等等的相关的呃。

一些大语言架构都在采用这种方式来完成。

![](img/35d23956928480939c04a4fdf5c355b6_57.png)

那么下面这个呢其实是给大家讲的叫prefix decoder，这个呢呃他又嗯被称为叫非因果解码器架构，非因果解码器吗。



![](img/35d23956928480939c04a4fdf5c355b6_59.png)

啊就是你听名字就能知道，他这边其实是把一些因果解码器的解码机制。

![](img/35d23956928480939c04a4fdf5c355b6_61.png)

做了一些修改，其实是啊对这块内容和我们的呃。

![](img/35d23956928480939c04a4fdf5c355b6_63.png)

前面call to decoder呢是不一样的，具体这边的话就像这儿介绍的一样，说前缀解码器啊，对于输入的前缀部分。



![](img/35d23956928480939c04a4fdf5c355b6_65.png)

使用了双下注意力进行编码，也就是说这部分他这边的话，其实主要是使用的这种呃。

![](img/35d23956928480939c04a4fdf5c355b6_67.png)

双向注意力制来进行编码的，然后对于这个输出部分呢，其实利用的还是一个单向的一个掩码注意力，然后利用词源的本身和前面的词，进行自回归的预测。



![](img/35d23956928480939c04a4fdf5c355b6_69.png)

大家需要注意的是，这个里边prefix decoder和我们的encoder。

![](img/35d23956928480939c04a4fdf5c355b6_71.png)

decoder就是编码器解码器，不同的是，我们前缀解码器。

![](img/35d23956928480939c04a4fdf5c355b6_73.png)

在编码和解码过程中是共享参数的，并没有像我们encoder和decoder一样。

![](img/35d23956928480939c04a4fdf5c355b6_75.png)

大家记得transformer那个图里边它们是独立的，只是通过一个中间的参数来进行传递。

![](img/35d23956928480939c04a4fdf5c355b6_77.png)

然后整个这个里边prefix decoder。

![](img/35d23956928480939c04a4fdf5c355b6_79.png)

它的一个呃代表的模型其实是gm系列的模型。

![](img/35d23956928480939c04a4fdf5c355b6_81.png)

然后pom也是啊，讲到这块之后呢，我们总结一下，前面给大家讲的这道面试题里边呃，主要讲的些知识点内容。



![](img/35d23956928480939c04a4fdf5c355b6_83.png)

大家会发现呃这三种就是prefix decoder。

![](img/35d23956928480939c04a4fdf5c355b6_85.png)

causal decoder和encoder decoder，区别在于那个attention mask不同，那么include decoder这边的话呃，大家记住主要代表的是T5这个模型。

然后在输入上采用的是双向注意力，对问题的编码理解也更加充分一些，然后适用的任务呢是偏理解的一些，LP任务就是理解方面的，这边会OK1些，然后在一些神成上它的效果比较差，而且训练效率呢。

因为他模型架构的这个特性训练效率比较低，然后call to decoder，这边的话其实是目前大语言模型的主流架构，它主要是通过GPT系列来带火它的。



![](img/35d23956928480939c04a4fdf5c355b6_87.png)

那么整体这边的话是通过这种，自回归的语言模型，然后训练和下游应用是完全一致的。

![](img/35d23956928480939c04a4fdf5c355b6_89.png)

严格遵守，只有后面的token才能看到前面的token这种规则，然后是种啊三角这种方式来逐渐的做掩码的。



![](img/35d23956928480939c04a4fdf5c355b6_91.png)

然后他的任务呢，主要是在文本生成效果上表现不错。

![](img/35d23956928480939c04a4fdf5c355b6_93.png)

然后他预训练效率高，zero shot能力更强，具有一些引线能力。

![](img/35d23956928480939c04a4fdf5c355b6_95.png)

那么prefix decoder呢它的代表呢其实就是JLM，然后整体里面特点是prefix部分与token能互相看到。



![](img/35d23956928480939c04a4fdf5c355b6_97.png)

然后剩的任务呢是在一些文本生成任务上。

![](img/35d23956928480939c04a4fdf5c355b6_99.png)